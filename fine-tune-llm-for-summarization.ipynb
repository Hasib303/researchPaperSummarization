{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:33:02.475008Z","iopub.execute_input":"2025-06-18T12:33:02.475579Z","iopub.status.idle":"2025-06-18T12:33:02.479249Z","shell.execute_reply.started":"2025-06-18T12:33:02.475555Z","shell.execute_reply":"2025-06-18T12:33:02.478555Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# 1. Load Dataset\ndataset = load_dataset(\"ccdv/arxiv-summarization\")\ndataset = dataset[\"train\"].select(range(1000))  # Use a smaller subset for faster training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:29:17.580968Z","iopub.execute_input":"2025-06-18T12:29:17.581275Z","iopub.status.idle":"2025-06-18T12:29:18.529624Z","shell.execute_reply.started":"2025-06-18T12:29:17.581255Z","shell.execute_reply":"2025-06-18T12:29:18.528905Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Preview a few samples\nfor i in range(3):\n    print(f\"\\nSample {i+1}:\")\n    print(\"Article:\\n\", dataset[i][\"article\"][:10000])  # first 1000 chars\n    print(\"\\nAbstract:\\n\", dataset[i][\"abstract\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:31:03.386149Z","iopub.execute_input":"2025-06-18T12:31:03.386406Z","iopub.status.idle":"2025-06-18T12:31:03.392793Z","shell.execute_reply.started":"2025-06-18T12:31:03.386389Z","shell.execute_reply":"2025-06-18T12:31:03.392011Z"}},"outputs":[{"name":"stdout","text":"\nSample 1:\nArticle:\n additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models . \n it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models . \n many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space @xmath0 , see e.g. @xcite . in the last years \n many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the references therein . of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded . \n this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded loss function , where results on upper bounds for the maxbias bias and on a bounded influence function are known , see e.g. @xcite for the general case and @xcite for additive models . \n therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @xmath1 for some @xmath2 which is a smoothness penalty but not a sparsity penalty , see e.g. @xcite . \n such regularized kernel based methods are now often called support vector machines ( svms ) , although the notation was historically used for such methods based on the special hinge loss function and for special kernels only , we refer to @xcite .    in this paper we address the open question , whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , if the assumption of an additive model is satisfied . \n our leading example covers learning rates for quantile regression based on the lipschitz continuous but non - differentiable pinball loss function , which is also called check function in the literature , see e.g. @xcite and @xcite for parametric quantile regression and @xcite , @xcite , and @xcite for kernel based quantile regression . \n we will not address the question how to check whether the assumption of an additive model is satisfied because this would be a topic of a paper of its own . \n of course , a practical approach might be to fit both models and compare their risks evaluated for test data . \n for the same reason we will also not cover sparsity . \n consistency of support vector machines generated by additive kernels for additive models was considered in @xcite . in this paper \n we establish learning rates for these algorithms . \n let us recall the framework with a complete separable metric space @xmath3 as the input space and a closed subset @xmath4 of @xmath5 as the output space . \n a borel probability measure @xmath6 on @xmath7 is used to model the learning problem and an independent and identically distributed sample @xmath8 is drawn according to @xmath6 for learning . \n a loss function @xmath9 is used to measure the quality of a prediction function @xmath10 by the local error @xmath11 . \n _ throughout the paper we assume that @xmath12 is measurable , @xmath13 , convex with respect to the third variable , and uniformly lipschitz continuous satisfying @xmath14 with a finite constant @xmath15 . \n _    support vector machines ( svms ) considered here are kernel - based regularization schemes in a reproducing kernel hilbert space ( rkhs ) @xmath0 generated by a mercer kernel @xmath16 . with a shifted loss function @xmath17 introduced for dealing \n even with heavy - tailed distributions as @xmath18 , they take the form @xmath19 where for a general borel measure @xmath20 on @xmath21 , the function @xmath22 is defined by @xmath23 where @xmath24 is a regularization parameter . \n the idea to shift a loss function has a long history , see e.g. @xcite in the context of m - estimators . \n it was shown in @xcite that @xmath22 is also a minimizer of the following optimization problem involving the original loss function @xmath12 if a minimizer exists : @xmath25    the additive model we consider consists of the _ input space decomposition _ \n @xmath26 with each @xmath27 a complete separable metric space and a _ hypothesis space _ \n @xmath28 where @xmath29 is a set of functions @xmath30 each of which is also identified as a map @xmath31 from @xmath3 to @xmath5 . \n hence the functions from @xmath32 take the additive form @xmath33 . \n we mention , that there is strictly speaking a notational problem here , because in the previous formula each quantity @xmath34 is an element of the set @xmath35 which is a subset of the full input space @xmath36 , @xmath37 , whereas in the definition of sample @xmath8 each quantity @xmath38 is an element of the full input space @xmath36 , where @xmath39 . \n because these notations will only be used in different places and because we do not expect any misunderstandings , we think this notation is easier and more intuitive than specifying these quantities with different symbols . \n the additive kernel @xmath40 is defined in terms of mercer kernels @xmath41 on @xmath27 as @xmath42 it generates an rkhs @xmath0 which can be written in terms of the rkhs @xmath43 generated by @xmath41 on @xmath27 corresponding to the form ( [ additive ] ) as @xmath44 with norm given by @xmath45 the norm of @xmath46 satisfies @xmath47    to illustrate advantages of additive models , we provide two examples of comparing additive with product kernels . \n the first example deals with gaussian rbf kernels . \n all proofs will be given in section [ proofsection ] . \n [ gaussadd ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and @xmath52.\\ ] ] the additive kernel @xmath53 is given by @xmath54 furthermore , the product kernel @xmath55 is the standard gaussian kernel given by @xmath56 define a gaussian function @xmath57 on @xmath58 ^ 2 $ ] depending only on one variable by @xmath59 then @xmath60 but @xmath61 where @xmath62 denotes the rkhs generated by the standard gaussian rbf kernel @xmath63 . \n the second example is about sobolev kernels . \n [ sobolvadd ] let @xmath64 , @xmath65 $ ] and @xmath58^s.$ ] let @xmath66 : = \\bigl\\{u\\in l_2([0,1 ] ) ; d^\\alpha u \\in l_2([0,1 ] ) \\mbox{~for~all~}|\\alpha|\\le 1\\bigr\\}\\ ] ] be the sobolev space consisting of all square integrable univariate functions whose derivative is also square integrable . \n it is an rkhs with a mercer kernel @xmath67 defined on @xmath68 ^ 2 $ ] . \n if we take all the mercer kernels @xmath69 to be @xmath67 , then @xmath70 $ ] for each @xmath71 . \n the additive kernel @xmath72 is also a mercer kernel and defines an rkhs @xmath73\\right\\}.\\ ] ] however , the multivariate sobolev space @xmath74^s)$ ] , consisting of all square integrable functions whose partial derivatives are all square integrable , contains discontinuous functions and is not an rkhs . \n denote the marginal distribution of @xmath6 on @xmath27 as @xmath75 . under the assumption that @xmath76 for each @xmath71 and that @xmath43 is dense in @xmath29 in the @xmath77-metric , it was proved in @xcite that @xmath78 in probability as long as @xmath79 satisfies @xmath80 and @xmath81 . \n the rest of the paper has the following structure . \n section [ ratessection ] contains our main results on learning rates for svms based on additive kernels . learning rates for quantile regression \n are treated as important special cases . \n section [ comparisonsection ] contains a comparison of our results with other learning rates published recently . \n section [ proofsection ] contains all the proofs and some results which can be interesting in their own . \n in this paper we provide some learning rates for the support vector machines generated by additive kernels for additive models which helps improve the quantitative understanding presented in @xcite . \n the rates are about asymptotic behaviors of the excess risk @xmath82 and take the form @xmath83 with @xmath84 . \n they will be stated under three kinds of conditions involving the hypothesis space @xmath0 , the measure @xmath6 , the loss @xmath12 , and the choice of the regularization parameter @xmath85 . \n the first condition is about the approximation ability of the hypothesis space @xmath0 . \n since the output function @xmath19 is from the hypothesis space , the learning rates of the learning algorithm depend on the approximation ability of the hypothesis space @xmath0 with respect to the optimal risk @xmath86 measured by the following approximation error . \n [ defapprox ] the approximation error of the triple @xmath87 is defined as @xmath88    to estimate the approximation error , we make an assumption about the minimizer of the risk @xmath89    for each @xmath90 , define the integral operator @xmath91 associated with the kernel @xmath41 by @xmath92 we mention that @xmath93 is a compact and positive operator on @xmath94 . hence we can find its normalized eigenpairs @xmath95 such that @xmath96 is an orthonormal basis of @xmath94 and @xmath97 as @xmath98 . fix @xmath99 . \n then we can define the @xmath100-th power @xmath101 of @xmath93 by @xmath102 this is a positive and bounded operator and its range is well - defined . \n the assumption @xmath103 means @xmath104 lies in this range . \n [ assumption1 ] we assume @xmath105 and @xmath106 where \n\nAbstract:\n additive models play an important role in semiparametric statistics . \n this paper gives learning rates for regularized kernel based methods for additive models . \n these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . \n additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .    * \n key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine .\n\nSample 2:\nArticle:\n the leptonic decays of a charged pseudoscalar meson @xmath7 are processes of the type @xmath8 , where @xmath9 , @xmath10 , or @xmath11 . because no strong interactions are present in the leptonic final state @xmath12 , such decays provide a clean way to probe the complex , strong interactions that bind the quark and antiquark within the initial - state meson . in these decays , strong interaction effects can be parametrized by a single quantity , @xmath13 , the pseudoscalar meson decay constant . \n the leptonic decay rate can be measured by experiment , and the decay constant can be determined by the equation ( ignoring radiative corrections ) @xmath14 where @xmath15 is the fermi coupling constant , @xmath16 is the cabibbo - kobayashi - maskawa ( ckm ) matrix  @xcite element , @xmath17 is the mass of the meson , and @xmath18 is the mass of the charged lepton . \n the quantity @xmath13 describes the amplitude for the @xmath19 and @xmath20-quarks within the @xmath21 to have zero separation , a condition necessary for them to annihilate into the virtual @xmath22 boson that produces the @xmath12 pair . \n the experimental determination of decay constants is one of the most important tests of calculations involving nonperturbative qcd . \n such calculations have been performed using various models  @xcite or using lattice qcd ( lqcd ) . \n the latter is now generally considered to be the most reliable way to calculate the quantity . \n knowledge of decay constants is important for describing several key processes , such as @xmath23 mixing , which depends on @xmath24 , a quantity that is also predicted by lqcd calculations . \n experimental determination  @xcite of @xmath24 with the leptonic decay of a @xmath25 meson is , however , very limited as the rate is highly suppressed due to the smallness of the magnitude of the relevant ckm matrix element @xmath26 . \n the charm mesons , @xmath27 and @xmath28 , are better instruments to study the leptonic decays of heavy mesons since these decays are either less ckm suppressed or favored , _ \n i.e. _ , @xmath29 and @xmath30 are much larger than @xmath31 . \n thus , the decay constants @xmath32 and @xmath33 determined from charm meson decays can be used to test and validate the necessary lqcd calculations applicable to the @xmath34-meson sector .    among \n the leptonic decays in the charm - quark sector , @xmath35 decays are more accessible since they are ckm favored . \n furthermore , the large mass of the @xmath11 lepton removes the helicity suppression that is present in the decays to lighter leptons . \n the existence of multiple neutrinos in the final state , however , makes measurement of this decay challenging . \n physics beyond the standard model ( sm ) might also affect leptonic decays of charmed mesons . \n depending on the non - sm features , the ratio of @xmath36 could be affected  @xcite , as could the ratio  @xcite @xmath37 . \n any of the individual widths might be increased or decreased . \n there is an indication of a discrepancy between the experimental determinations  @xcite of @xmath33 and the most recent precision lqcd calculation  @xcite . \n this disagreement is particularly puzzling since the cleo - c determination  @xcite of @xmath32 agrees well with the lqcd calculation  @xcite of that quantity . some  @xcite conjecture that this discrepancy may be explained by a charged higgs boson or a leptoquark .    in this article \n , we report an improved measurement of the absolute branching fraction of the leptonic decay @xmath0 ( charge - conjugate modes are implied ) , with @xmath1 , from which we determine the decay constant @xmath33 . \n we use a data sample of @xmath38 events provided by the cornell electron storage ring ( cesr ) and collected by the cleo - c detector at the center - of - mass ( cm ) energy @xmath39 mev , near @xmath3 peak production  @xcite . \n the data sample consists of an integrated luminosity of @xmath40 @xmath41 containing @xmath42 @xmath3 pairs . \n we have previously reported  @xcite measurements of @xmath43 and @xmath0 with a subsample of these data . a companion article  @xcite reports measurements of @xmath33 from @xmath43 and @xmath0 , with @xmath44 , using essentially the same data sample as the one used in this measurement . \n the cleo - c detector  @xcite is a general - purpose solenoidal detector with four concentric components utilized in this measurement : a small - radius six - layer stereo wire drift chamber , a 47-layer main drift chamber , a ring - imaging cherenkov ( rich ) detector , and an electromagnetic calorimeter consisting of 7800 csi(tl ) crystals . \n the two drift chambers operate in a @xmath45  t magnetic field and provide charged particle tracking in a solid angle of @xmath46% of @xmath47 . \n the chambers achieve a momentum resolution of @xmath48% at @xmath49  gev/@xmath50 . \n the main drift chamber also provides specific - ionization ( @xmath51 ) measurements that discriminate between charged pions and kaons . \n the rich detector covers approximately @xmath52% of @xmath47 and provides additional separation of pions and kaons at high momentum . \n the photon energy resolution of the calorimeter is @xmath53% at @xmath54  gev and @xmath55% at @xmath56  mev . \n electron identification is based on a likelihood variable that combines the information from the rich detector , @xmath51 , and the ratio of electromagnetic shower energy to track momentum ( @xmath57 ) . \n we use a geant - based  @xcite monte carlo ( mc ) simulation program to study efficiency of signal - event selection and background processes . \n physics events are generated by evtgen  @xcite , tuned with much improved knowledge of charm decays  @xcite , and final - state radiation ( fsr ) is modeled by the photos  @xcite program . \n the modeling of initial - state radiation ( isr ) is based on cross sections for @xmath3 production at lower energies obtained from the cleo - c energy scan  @xcite near the cm energy where we collect the sample . \n the presence of two @xmath58 mesons in a @xmath3 event allows us to define a single - tag ( st ) sample in which a @xmath58 is reconstructed in a hadronic decay mode and a further double - tagged ( dt ) subsample in which an additional @xmath59 is required as a signature of @xmath60 decay , the @xmath59 being the daughter of the @xmath60 . \n the @xmath61 reconstructed in the st sample can be either primary or secondary from @xmath62 ( or @xmath63 ) . \n the st yield can be expressed as @xmath64 where @xmath65 is the produced number of @xmath3 pairs , @xmath66 is the branching fraction of hadronic modes used in the st sample , and @xmath67 is the st efficiency . \n the @xmath68 counts the candidates , not events , and the factor of 2 comes from the sum of @xmath28 and @xmath61 tags . \n our double - tag ( dt ) sample is formed from events with only a single charged track , identified as an @xmath69 , in addition to a st . \n the yield can be expressed as @xmath70 where @xmath71 is the leptonic decay branching fraction , including the subbranching fraction of @xmath1 decay , @xmath72 is the efficiency of finding the st and the leptonic decay in the same event . from the st and dt yields we can obtain an absolute branching fraction of the leptonic decay @xmath71 , without needing to know the integrated luminosity or the produced number of @xmath3 pairs , @xmath73 where @xmath74 ( @xmath75 ) is the effective signal efficiency . because of the large solid angle acceptance with high segmentation of the cleo - c detector and the low multiplicity of the events with which we are concerned , @xmath76 , where @xmath77 is the leptonic decay efficiency . \n hence , the ratio @xmath78 is insensitive to most systematic effects associated with the st , and the signal branching fraction @xmath71 obtained using this procedure is nearly independent of the efficiency of the tagging mode .      to minimize systematic uncertainties , we tag using three two - body hadronic decay modes with only charged particles in the final state . \n the three st modes and @xmath79 are shorthand labels for @xmath80 events within mass windows ( described below ) of the @xmath81 peak in @xmath82 and the @xmath83 peak in @xmath84 , respectively . \n no attempt is made to separate these resonance components in the @xmath85 dalitz plot . ] \n are @xmath86 , @xmath79 , and @xmath87 . \n using these tag modes also helps to reduce the tag bias which would be caused by the correlation between the tag side and the signal side reconstruction if tag modes with high multiplicity and large background were used . \n the effect of the tag bias @xmath88 can be expressed in terms of the signal efficiency @xmath74 defined by @xmath89 where @xmath90 is the st efficiency when the recoiling system is the signal leptonic decay with single @xmath59 in the other side of the tag . \n as the general st efficiency @xmath67 , when the recoiling system is any possible @xmath91 decays , will be lower than the @xmath90 , sizable tag bias could be introduced if the multiplicity of the tag mode were high , or the tag mode were to include neutral particles in the final state . as shown in sec . \n [ sec : results ] , this effect is negligible in our chosen clean tag modes . \n the @xmath92 decay is reconstructed by combining oppositely charged tracks that originate from a common vertex and that have an invariant mass within @xmath93 mev of the nominal mass  @xcite . \n we require the resonance decay to satisfy the following mass windows around the nominal masses  @xcite : @xmath94 ( @xmath95 mev ) and @xmath96 ( @xmath97 mev ) . \n we require the momenta of charged particles to be @xmath56 mev or greater to suppress the slow pion background from @xmath98 decays ( through @xmath99 ) . \n we identify a st by using the invariant mass of the tag @xmath100 and recoil mass against the tag @xmath101 . \n the recoil mass is defined as @xmath102 where @xmath103 is the net four - momentum of the @xmath4 beam , taking\n\nAbstract:\n we have studied the leptonic decay @xmath0 , via the decay channel @xmath1 , using a sample of tagged @xmath2 decays collected near the @xmath3 peak production energy in @xmath4 collisions with the cleo - c detector . \n we obtain @xmath5 and determine the decay constant @xmath6  mev , where the first uncertainties are statistical and the second are systematic .\n\nSample 3:\nArticle:\n the transport properties of nonlinear non - equilibrium dynamical systems are far from well - understood@xcite . \n consider in particular so - called ratchet systems which are asymmetric periodic potentials where an ensemble of particles experience directed transport@xcite . \n the origins of the interest in this lie in considerations about extracting useful work from unbiased noisy fluctuations as seems to happen in biological systems@xcite . \n recently attention has been focused on the behavior of deterministic chaotic ratchets@xcite as well as hamiltonian ratchets@xcite . \n chaotic systems are defined as those which are sensitively dependent on initial conditions . whether chaotic or not , the behavior of nonlinear systems  including the transition from regular to chaotic behavior  is in general sensitively dependent on the parameters of the system . \n that is , the phase - space structure is usually relatively complicated , consisting of stability islands embedded in chaotic seas , for examples , or of simultaneously co - existing attractors . \n this can change significantly as parameters change . \n for example , stability islands can merge into each other , or break apart , and the chaotic sea itself may get pinched off or otherwise changed , or attractors can change symmetry or bifurcate . \n this means that the transport properties can change dramatically as well . \n a few years ago , mateos@xcite considered a specific ratchet model with a periodically forced underdamped particle . \n he looked at an ensemble of particles , specifically the velocity for the particles , averaged over time and the entire ensemble . \n he showed that this quantity , which is an intuitively reasonable definition of ` the current ' , could be either positive or negative depending on the amplitude @xmath0 of the periodic forcing for the system . at the same time , there exist ranges in @xmath0 where the trajectory of an individual particle displays chaotic dynamics . \n mateos conjectured a connection between these two phenomena , specifically that the reversal of current direction was correlated with a bifurcation from chaotic to periodic behavior in the trajectory dynamics . \n even though it is unlikely that such a result would be universally valid across all chaotic deterministic ratchets , it would still be extremely useful to have general heuristic rules such as this . \n these organizing principles would allow some handle on characterizing the many different kinds of behavior that are possible in such systems . \n a later investigation@xcite of the mateos conjecture by barbi and salerno , however , argued that it was not a valid rule even in the specific system considered by mateos . \n they presented results showing that it was possible to have current reversals in the absence of bifurcations from periodic to chaotic behavior . \n they proposed an alternative origin for the current reversal , suggesting it was related to the different stability properties of the rotating periodic orbits of the system . \n these latter results seem fundamentally sensible . however , this paper based its arguments about currents on the behavior of a _ single _ particle as opposed to an ensemble . \n this implicitly assumes that the dynamics of the system are ergodic . \n this is not true in general for chaotic systems of the type being considered . in particular , there can be extreme dependence of the result on the statistics of the ensemble being considered . \n this has been pointed out in earlier studies  @xcite which laid out a detailed methodology for understanding transport properties in such a mixed regular and chaotic system . depending on specific parameter value , the particular system under consideration has multiple coexisting periodic or chaotic attractors or a mixture of both . \n it is hence appropriate to understand how a probability ensemble might behave in such a system . \n the details of the dependence on the ensemble are particularly relevant to the issue of the possible experimental validation of these results , since experiments are always conducted , by virtue of finite - precision , over finite time and finite ensembles . \n it is therefore interesting to probe the results of barbi and salerno with regard to the details of the ensemble used , and more formally , to see how ergodicity alters our considerations about the current , as we do in this paper . \n we report here on studies on the properties of the current in a chaotic deterministic ratchet , specifically the same system as considered by mateos@xcite and barbi and salerno@xcite . \n we consider the impact of different kinds of ensembles of particles on the current and show that the current depends significantly on the details of the initial ensemble . \n we also show that it is important to discard transients in quantifying the current . \n this is one of the central messages of this paper : broad heuristics are rare in chaotic systems , and hence it is critical to understand the ensemble - dependence in any study of the transport properties of chaotic ratchets . \n having established this , we then proceed to discuss the connection between the bifurcation diagram for individual particles and the behavior of the current . \n we find that while we disagree with many of the details of barbi and salerno s results , the broader conclusion still holds . \n that is , it is indeed possible to have current reversals in the absence of bifurcations from chaos to periodic behavior as well as bifurcations without any accompanying current reversals . \n the result of our investigation is therefore that the transport properties of a chaotic ratchet are not as simple as the initial conjecture . \n however , we do find evidence for a generalized version of mateos s conjecture . \n that is , in general , bifurcations for trajectory dynamics as a function of system parameter seem to be associated with abrupt changes in the current . \n depending on the specific value of the current , these abrupt changes may lead the net current to reverse direction , but not necessarily so . \n we start below with a preparatory discussion necessary to understand the details of the connection between bifurcations and current reversal , where we discuss the potential and phase - space for single trajectories for this system , where we also define a bifurcation diagram for this system . in the next section , \n we discuss the subtleties of establishing a connection between the behavior of individual trajectories and of ensembles . \n after this , we are able to compare details of specific trajectory bifurcation curves with current curves , and thus justify our broader statements above , after which we conclude . \n the goal of these studies is to understand the behavior of general chaotic ratchets . \n the approach taken here is that to discover heuristic rules we must consider specific systems in great detail before generalizing . \n we choose the same @xmath1-dimensional ratchet considered previously by mateos@xcite , as well as barbi and salerno@xcite . \n we consider an ensemble of particles moving in an asymmetric periodic potential , driven by a periodic time - dependent external force , where the force has a zero time - average . \n there is no noise in the system , so it is completely deterministic , although there is damping . \n the equations of motion for an individual trajectory for such a system are given in dimensionless variables by @xmath2 where the periodic asymmetric potential can be written in the form @xmath3 + \\frac{1}{4 } \\sin [ 4\\pi ( x -x_0 ) ] \\bigg ] .\\ ] ] in this equation @xmath4 have been introduced for convenience such that one potential minimum exists at the origin with @xmath5 and the term @xmath6 . \n ( a ) classical phase space for the unperturbed system . for @xmath7 , @xmath8 , \n two chaotic attractors emerge with @xmath9 ( b ) @xmath10 ( c ) and a period four attractor consisting of the four centers of the circles with @xmath11.,title=\"fig:\",width=302 ]    the phase - space of the undamped undriven ratchet  the system corresponding to the unperturbed potential @xmath12  looks like a series of asymmetric pendula . \n that is , individual trajectories have one of following possible time - asymptotic behaviors : ( i ) inside the potential wells , trajectories and all their properties oscillate , leading to zero net transport . outside the wells , the trajectories either ( ii ) librate to the right or ( iii ) to the left , with corresponding net transport depending upon initial conditions . \n there are also ( iv ) trajectories on the separatrices between the oscillating and librating orbits , moving between unstable fixed points in infinite time , as well as the unstable and stable fixed points themselves , all of which constitute a set of negligible measure .    when damping is introduced via the @xmath13-dependent term in eq . \n [ eq : dyn ] , it makes the stable fixed points the only attractors for the system . \n when the driving is turned on , the phase - space becomes chaotic with the usual phenomena of intertwining separatrices and resulting homoclinic tangles . \n the dynamics of individual trajectories in such a system are now very complicated in general and depend sensitively on the choice of parameters and initial conditions . \n we show snapshots of the development of this kind of chaos in the set of poincar sections fig . \n ( [ figure1]b , c ) together with a period - four orbit represented by the center of the circles .    a broad characterization of the dynamics of the problem as a function of a parameter ( @xmath14 or @xmath15 ) emerges in a bifurcation diagram \n . this can be constructed in several different and essentially equivalent ways . \n the relatively standard form that we use proceeds as follows : first choose the bifurcation parameter ( let us say @xmath0 ) and correspondingly choose fixed values of @xmath16 , and start with a given value for @xmath17 . \n now iterate an i\n\nAbstract:\n in 84 , 258 ( 2000 ) , mateos conjectured that current reversal in a classical deterministic ratchet is associated with bifurcations from chaotic to periodic regimes . \n this is based on the comparison of the current and the bifurcation diagram as a function of a given parameter for a periodic asymmetric potential . \n barbi and salerno , in 62 , 1988 ( 2000 ) , have further investigated this claim and argue that , contrary to mateos claim , current reversals can occur also in the absence of bifurcations . \n barbi and salerno s studies are based on the dynamics of one particle rather than the statistical mechanics of an ensemble of particles moving in the chaotic system . \n the behavior of ensembles can be quite different , depending upon their characteristics , which leaves their results open to question . in this paper we present results from studies showing how the current depends on the details of the ensemble \n used to generate it , as well as conditions for convergent behavior ( that is , independent of the details of the ensemble ) . \n we are then able to present the converged current as a function of parameters , in the same system as mateos as well as barbi and salerno . \n we show evidence for current reversal without bifurcation , as well as bifurcation without current reversal . \n we conjecture that it is appropriate to correlate abrupt changes in the current with bifurcation , rather than current reversals , and show numerical evidence for our claims .\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# 2. Tokenizer and Model Setup\nmodel_checkpoint = \"google/long-t5-tglobal-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\nmodel = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:33:59.363958Z","iopub.execute_input":"2025-06-18T12:33:59.364477Z","iopub.status.idle":"2025-06-18T12:34:06.311822Z","shell.execute_reply.started":"2025-06-18T12:33:59.364455Z","shell.execute_reply":"2025-06-18T12:34:06.310946Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/851 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d21e3e9cce4c12a30b3e7269a3ae75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23adf310e8ed438e91422d68f940af98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0fcfc2d79b41c4b72088920f6f43dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b27b06367944a73879d8eb00c768e8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"536c9682374e4e5a83778e2c7d65776f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab83f1ad62c84505b18e833d4072e41b"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n#     # model = torch.nn.DataParallel(model)\n\n# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:22:16.277894Z","iopub.execute_input":"2025-06-18T12:22:16.278613Z","iopub.status.idle":"2025-06-18T12:22:16.281810Z","shell.execute_reply.started":"2025-06-18T12:22:16.278586Z","shell.execute_reply":"2025-06-18T12:22:16.281043Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 3. Preprocessing Function\ndef preprocess_function(examples):\n    inputs = [\"summarize: \" + article for article in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(text_target=examples[\"abstract\"], max_length=150, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:34:26.087465Z","iopub.execute_input":"2025-06-18T12:34:26.087786Z","iopub.status.idle":"2025-06-18T12:34:26.092235Z","shell.execute_reply.started":"2025-06-18T12:34:26.087742Z","shell.execute_reply":"2025-06-18T12:34:26.091524Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# 4. Tokenize Dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:34:39.308196Z","iopub.execute_input":"2025-06-18T12:34:39.308482Z","iopub.status.idle":"2025-06-18T12:34:51.761918Z","shell.execute_reply.started":"2025-06-18T12:34:39.308461Z","shell.execute_reply":"2025-06-18T12:34:51.761201Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7cfda887d94113bf9e0cbc347abc57"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# 5. Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"longt5-arxiv-summarization-run\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    warmup_steps=10,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_total_limit=2,\n    save_steps=500,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\",\n    remove_unused_columns=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:35:04.237026Z","iopub.execute_input":"2025-06-18T12:35:04.237612Z","iopub.status.idle":"2025-06-18T12:35:04.270639Z","shell.execute_reply.started":"2025-06-18T12:35:04.237590Z","shell.execute_reply":"2025-06-18T12:35:04.270114Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# 6. Trainer Setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:35:14.077045Z","iopub.execute_input":"2025-06-18T12:35:14.077350Z","iopub.status.idle":"2025-06-18T12:35:14.096333Z","shell.execute_reply.started":"2025-06-18T12:35:14.077329Z","shell.execute_reply":"2025-06-18T12:35:14.095750Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(\"🚀 Starting training...\")\n!nvidia-smi ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:35:22.539725Z","iopub.execute_input":"2025-06-18T12:35:22.540343Z","iopub.status.idle":"2025-06-18T12:35:22.887059Z","shell.execute_reply.started":"2025-06-18T12:35:22.540321Z","shell.execute_reply":"2025-06-18T12:35:22.886331Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting training...\nWed Jun 18 12:35:22 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   72C    P0             30W /   70W |    5943MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   77C    P0             35W /   70W |    3565MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# 7. Train\ntrainer.train()\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:35:28.681254Z","iopub.execute_input":"2025-06-18T12:35:28.681883Z","iopub.status.idle":"2025-06-18T12:51:16.054187Z","shell.execute_reply.started":"2025-06-18T12:35:28.681853Z","shell.execute_reply":"2025-06-18T12:51:16.053000Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 15:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>5.144900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.022100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.372800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.171000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.011300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.872800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.839500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.746000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.754100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.692400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.626400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.697400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.746400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.674500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.674700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Wed Jun 18 12:51:15 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   73C    P0             30W /   70W |    6629MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   68C    P0             32W /   70W |    2721MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# 8. Inference on a Sample\nsample_text = dataset[0][\"article\"]\ninput_ids = tokenizer(\"summarize: \" + sample_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(\"cuda\")\noutput_ids = model.module.generate(input_ids, max_length=150, num_beams=2) if hasattr(model, \"module\") else model.generate(input_ids, max_length=150, num_beams=2)\nsummary = tokenizer.decode(output_ids[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:53:26.868147Z","iopub.execute_input":"2025-06-18T12:53:26.868660Z","iopub.status.idle":"2025-06-18T12:53:30.415292Z","shell.execute_reply.started":"2025-06-18T12:53:26.868629Z","shell.execute_reply":"2025-06-18T12:53:30.414708Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(\"\\nGenerated Summary:\\n\", summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:53:36.878628Z","iopub.execute_input":"2025-06-18T12:53:36.878959Z","iopub.status.idle":"2025-06-18T12:53:36.883327Z","shell.execute_reply.started":"2025-06-18T12:53:36.878924Z","shell.execute_reply":"2025-06-18T12:53:36.882657Z"}},"outputs":[{"name":"stdout","text":"\nGenerated Summary:\n we study the case of regularized kernel based methods based on a general convex and on a bounded loss function on a general kernel . we consider the case of regularized kernel based methods based on a general convex and on a bounded loss function on a general kernel . we also consider a general case of regularized kernel based methods based on a general convex and on a bounded loss function on a general kernel . we also consider the case of regularized kernel based methods based on a general convex and on a bounded loss function .\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:55:47.857275Z","iopub.execute_input":"2025-06-18T12:55:47.858036Z","iopub.status.idle":"2025-06-18T12:55:52.775841Z","shell.execute_reply.started":"2025-06-18T12:55:47.858013Z","shell.execute_reply":"2025-06-18T12:55:52.775080Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=16c320310333a436c1ff3ba1a7bcd2159bae6fff6de919f35c91ab1613c85c58\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Evaluate with ROUGE\nimport evaluate\nrouge = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:55:56.362819Z","iopub.execute_input":"2025-06-18T12:55:56.363676Z","iopub.status.idle":"2025-06-18T12:55:56.606461Z","shell.execute_reply.started":"2025-06-18T12:55:56.363640Z","shell.execute_reply":"2025-06-18T12:55:56.605963Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"preds, refs = [], []\nfor ex in dataset.select(range(10)):\n    input_ids = tokenizer(\"summarize: \" + ex[\"article\"], return_tensors=\"pt\", max_length=2048, truncation=True).input_ids.to(\"cuda\")\n    output_ids = model.generate(input_ids, max_length=256, num_beams=2)\n    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    preds.append(pred)\n    refs.append(ex[\"abstract\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:56:12.831414Z","iopub.execute_input":"2025-06-18T12:56:12.832151Z","iopub.status.idle":"2025-06-18T12:57:19.333060Z","shell.execute_reply.started":"2025-06-18T12:56:12.832124Z","shell.execute_reply":"2025-06-18T12:57:19.332206Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"results = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\nprint(\"\\nROUGE Evaluation (10 samples):\\n\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:57:38.518682Z","iopub.execute_input":"2025-06-18T12:57:38.519438Z","iopub.status.idle":"2025-06-18T12:57:38.990124Z","shell.execute_reply.started":"2025-06-18T12:57:38.519412Z","shell.execute_reply":"2025-06-18T12:57:38.989352Z"}},"outputs":[{"name":"stdout","text":"\nROUGE Evaluation (10 samples):\n {'rouge1': 0.263204747006002, 'rouge2': 0.06315976492163909, 'rougeL': 0.18880264946761055, 'rougeLsum': 0.22619862034347016}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}